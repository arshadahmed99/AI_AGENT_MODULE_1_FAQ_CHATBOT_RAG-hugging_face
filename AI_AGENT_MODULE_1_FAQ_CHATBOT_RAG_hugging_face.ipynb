{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyIZ8WPuVRMX",
        "outputId": "f5fd3e3c-5ef2-4c26-efb6-fca8f8c6bb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 1 documents\n",
            "‚úÖ Split into 18 chunks\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Install dependencies (first time only)\n",
        "!pip install -q langchain faiss-cpu pypdf langchain-community\n",
        "!pip install -q sentence-transformers transformers bs4\n",
        "\n",
        "# üì• Step 1: Load and Chunk Web Data\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://ai.pydantic.dev/\",),\n",
        ")\n",
        "docs = loader.load()\n",
        "print(f\"‚úÖ Loaded {len(docs)} documents\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"‚úÖ Split into {len(chunks)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "print(\"‚úÖ FAISS Vectorstore created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBwgZXT2WCx_",
        "outputId": "b9403a36-6022-4249-a0a1-eba80d2b4ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS Vectorstore created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Local QA model (downloads once, then cached)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "print(\"‚úÖ Local QA pipeline ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TwBXoe2XfTj",
        "outputId": "72083f8d-f637-444f-87c8-adab3902c065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Local QA pipeline ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def local_rag_qa(query, top_k=3):\n",
        "    # Search FAISS for relevant chunks\n",
        "    docs = vectorstore.similarity_search(query, k=top_k)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Run QA over combined context\n",
        "    result = qa_pipeline({\n",
        "        \"question\": query,\n",
        "        \"context\": context\n",
        "    })\n",
        "\n",
        "    print(f\"\\n‚ùì Question: {query}\")\n",
        "    print(f\"üìé Answer: {result['answer']}\\n\")\n",
        "    print(\"üìö Top Contexts:\")\n",
        "    for doc in docs:\n",
        "        print(\"-\", doc.page_content[:150], \"...\\n\")\n"
      ],
      "metadata": {
        "id": "2Z2yFIfRXhpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is PydanticAI and why was it created?\",\n",
        "    \"How is PydanticAI similar to FastAPI in terms of developer experience?\",\n",
        "    \"What makes PydanticAI type-safe and structured?\",\n",
        "    \"How does PydanticAI support streaming and debugging?\",\n",
        "    \"What is llms.txt?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    local_rag_qa(q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJcxlI7TXm-k",
        "outputId": "72115163-9957-435f-ee73-4f06da568b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Question: What is PydanticAI and why was it created?\n",
            "üìé Answer: to make it less painful to\n",
            "  build production grade applications with Generative AI\n",
            "\n",
            "üìö Top Contexts:\n",
            "- PydanticAI\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          Skip to content\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            PydanticAI\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "            \n",
            "            ...\n",
            "\n",
            "- This Pydantic model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that  ...\n",
            "\n",
            "- pydantic_graph.exceptions\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.dataset\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.evaluators\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.reporting ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Question: How is PydanticAI similar to FastAPI in terms of developer experience?\n",
            "üìé Answer: Python agent framework\n",
            "\n",
            "üìö Top Contexts:\n",
            "- FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of Pydantic.\n",
            "Similarly, virtually every ...\n",
            "\n",
            "- pydantic_graph.exceptions\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.dataset\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.evaluators\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.reporting ...\n",
            "\n",
            "- PydanticAI\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          Skip to content\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            PydanticAI\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "            \n",
            "            ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Question: What makes PydanticAI type-safe and structured?\n",
            "üìé Answer: to make it less painful to\n",
            "  build production grade applications with Generative AI\n",
            "\n",
            "üìö Top Contexts:\n",
            "- This Pydantic model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that  ...\n",
            "\n",
            "- PydanticAI\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          Skip to content\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            PydanticAI\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "            \n",
            "            ...\n",
            "\n",
            "- pydantic_graph.exceptions\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.dataset\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.evaluators\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.reporting ...\n",
            "\n",
            "\n",
            "‚ùì Question: How does PydanticAI support streaming and debugging?\n",
            "üìé Answer: integrates with Pydantic Logfire\n",
            "\n",
            "üìö Top Contexts:\n",
            "- FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of Pydantic.\n",
            "Similarly, virtually every ...\n",
            "\n",
            "- pydantic_graph.exceptions\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.dataset\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.evaluators\n",
            "    \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    pydantic_evals.reporting ...\n",
            "\n",
            "- As of today, these files cannot be natively leveraged by LLM frameworks or IDEs. Alternatively,\n",
            "an MCP server can be implemented to properly parse the ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Question: What is llms.txt?\n",
            "üìé Answer: a file containing a brief description\n",
            "  of the project\n",
            "\n",
            "üìö Top Contexts:\n",
            "- As of today, these files cannot be natively leveraged by LLM frameworks or IDEs. Alternatively,\n",
            "an MCP server can be implemented to properly parse the ...\n",
            "\n",
            "- Configure logfire, this will fail if project is not set up.\n",
            "In our demo, DatabaseConn uses asyncpg to connect to a PostgreSQL database, so logfire.ins ...\n",
            "\n",
            "- Dynamic system prompts can be registered with the @agent.system_prompt decorator, and can make use of dependency injection. Dependencies are carried v ...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}